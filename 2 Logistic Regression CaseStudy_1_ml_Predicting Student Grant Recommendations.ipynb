{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective : Student Grant Recommendation\n",
    "\n",
    "You have historical student performance data and their grant recommendation outcomes in the form of a comma separated value file named student_records.csv. Each data sample consists of the following attributes.\n",
    "\n",
    ">• Name (the student name)\n",
    "\n",
    ">• OverallGrade (overall grade obtained)\n",
    "\n",
    ">• Obedient (whether they were diligent during their course of stay)\n",
    "\n",
    ">• ResearchScore (marks obtained in their research work)\n",
    "\n",
    ">• ProjectScore (marks obtained in the project)\n",
    "\n",
    ">• Recommend (whether they got the grant recommendation)\n",
    "\n",
    "Your main objective is to build a predictive model based on this data such that you can predict for any future student whether they will be recommended for the grant based on their performance attributes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 1: Data Retrieval\n",
    "----------------------\n",
    "Here, we will leverage the pandas framework to retrieve the data from the CSV file. The following snippet shows us how to retrieve the data and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/52/ca/f986280226b62da6ae5474589a369b0d240f9a61a99144a501b45f108883/pandas-0.25.3-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting numpy>=1.13.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/cf/7cea38d32df6087d7c15bca8edef0be82e0d957119e9dafd7052dc6192f0/numpy-1.17.4-cp38-cp38-macosx_10_9_x86_64.whl (15.1MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas) (1.13.0)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "Successfully installed numpy-1.17.4 pandas-0.25.3 pytz-2019.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>OverallGrade</th>\n",
       "      <th>Obedient</th>\n",
       "      <th>ResearchScore</th>\n",
       "      <th>ProjectScore</th>\n",
       "      <th>Recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Henry</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>85</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holmes</td>\n",
       "      <td>B</td>\n",
       "      <td>Y</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marvin</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Simon</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>92</td>\n",
       "      <td>79</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Robert</td>\n",
       "      <td>B</td>\n",
       "      <td>Y</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trent</td>\n",
       "      <td>C</td>\n",
       "      <td>Y</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name OverallGrade Obedient  ResearchScore  ProjectScore Recommend\n",
       "0   Henry            A        Y             90            85       Yes\n",
       "1    John            C        N             85            51       Yes\n",
       "2   David            F        N             10            17        No\n",
       "3  Holmes            B        Y             75            71        No\n",
       "4  Marvin            E        N             20            30        No\n",
       "5   Simon            A        Y             92            79       Yes\n",
       "6  Robert            B        Y             60            59        No\n",
       "7   Trent            C        Y             75            33        No"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "                            #--turn of warning messages\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#--get data\n",
    "df = pd.read_csv('./datasets_n_images/datasets_module_1/student_records.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 2: Data Preparation\n",
    "------------------------\n",
    "Based on the dataset (above), we do not have any data errors or missing values, hence we will mainly focus on feature engineering and scaling in this section."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 3 : Feature Extraction and Engineering\n",
    "-------------------------------------------\n",
    "Let’s start by extracting the existing features from the dataset and the outcomes; in separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(training_features):  <class 'pandas.core.frame.DataFrame'>\n",
      "--------------\n",
      "\n",
      "\n",
      "training_features:\n",
      "  OverallGrade Obedient  ResearchScore  ProjectScore\n",
      "0            A        Y             90            85\n",
      "1            C        N             85            51\n",
      "2            F        N             10            17\n",
      "3            B        Y             75            71\n",
      "4            E        N             20            30\n",
      "5            A        Y             92            79\n",
      "6            B        Y             60            59\n",
      "7            C        Y             75            33\n",
      "----------------\n",
      "\n",
      "outcome_labels:\n",
      "  Recommend\n",
      "0       Yes\n",
      "1       Yes\n",
      "2        No\n",
      "3        No\n",
      "4        No\n",
      "5       Yes\n",
      "6        No\n",
      "7        No\n"
     ]
    }
   ],
   "source": [
    "#--get features and corresponding outcomes\n",
    "feature_names = ['OverallGrade', 'Obedient', 'ResearchScore', 'ProjectScore']\n",
    "training_features = df[feature_names] \n",
    "#OR above two lines could also be written as below:\n",
    "#training_features = df['OverallGrade', 'Obedient', 'ResearchScore', 'ProjectScore']\n",
    "\n",
    "\n",
    "print(\"type(training_features): \",type(training_features))\n",
    "print(\"--------------\\n\")\n",
    "outcome_name = ['Recommend']\n",
    "outcome_labels = df[outcome_name]\n",
    "print(\"\\ntraining_features:\")\n",
    "print(training_features)\n",
    "print(\"----------------\")\n",
    "print(\"\\noutcome_labels:\")\n",
    "print(outcome_labels)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have extracted our initial available features from the data and their corresponding outcome labels, let’s separate out our available features based on their type (numerical and categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--list down features based on type\n",
    "numeric_feature_names = ['ResearchScore', 'ProjectScore']\n",
    "categoricial_feature_names = ['OverallGrade', 'Obedient']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now use a standard scalar from scikit-learn to scale or normalize our two numeric scorebased attributes using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sklearn) (0.22)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.17.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.14.0)\n",
      "Collecting scikit-datasets\n",
      "  Downloading https://files.pythonhosted.org/packages/89/f1/858de12daffb183c368412425905d4b6e0fdda4d9d119dd2158123d2fdaa/scikit_datasets-0.1.36-py3-none-any.whl\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-datasets) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-datasets) (0.22)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-datasets) (1.17.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->scikit-datasets) (0.14.0)\n",
      "Installing collected packages: scikit-datasets\n",
      "Successfully installed scikit-datasets-0.1.36\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install scikit-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OverallGrade Obedient  ResearchScore  ProjectScore\n",
      "0            A        Y       0.899583      1.376650\n",
      "1            C        N       0.730648     -0.091777\n",
      "2            F        N      -1.803390     -1.560203\n",
      "3            B        Y       0.392776      0.772004\n",
      "4            E        N      -1.465519     -0.998746\n",
      "5            A        Y       0.967158      1.117516\n",
      "6            B        Y      -0.114032      0.253735\n",
      "7            C        Y       0.392776     -0.869179\n"
     ]
    }
   ],
   "source": [
    "#--scale or normalize our two numeric score-based attributes\n",
    "from sklearn.preprocessing import StandardScaler    #sklearn.preprocessing is the package used \n",
    "ss = StandardScaler() #creating object of StandardScaler \n",
    "\n",
    "# fit scaler on numeric features\n",
    "ss.fit(training_features[numeric_feature_names])  #fit means it will analyse the data\n",
    "\n",
    "# scale numeric features now\n",
    "training_features[numeric_feature_names] = ss.transform(training_features[numeric_feature_names]) #transform means it will find log\n",
    "\n",
    "# view updated feature-set\n",
    "print(training_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# to suppress warnings (as in the above case)\n",
    "# import warnings; warnings.simplefilter('ignore')  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have successfully scaled our numeric features, let’s handle our categorical features and carry out the necessary feature engineering needed based on the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResearchScore  ProjectScore  OverallGrade_A  OverallGrade_B  \\\n",
      "0       0.899583      1.376650               1               0   \n",
      "1       0.730648     -0.091777               0               0   \n",
      "2      -1.803390     -1.560203               0               0   \n",
      "3       0.392776      0.772004               0               1   \n",
      "4      -1.465519     -0.998746               0               0   \n",
      "5       0.967158      1.117516               1               0   \n",
      "6      -0.114032      0.253735               0               1   \n",
      "7       0.392776     -0.869179               0               0   \n",
      "\n",
      "   OverallGrade_C  OverallGrade_E  OverallGrade_F  Obedient_N  Obedient_Y  \n",
      "0               0               0               0           0           1  \n",
      "1               1               0               0           1           0  \n",
      "2               0               0               1           1           0  \n",
      "3               0               0               0           0           1  \n",
      "4               0               1               0           1           0  \n",
      "5               0               0               0           0           1  \n",
      "6               0               0               0           0           1  \n",
      "7               1               0               0           0           1  \n"
     ]
    }
   ],
   "source": [
    "#--Engineering Categorical Features\n",
    "training_features = pd.get_dummies(training_features, columns=categoricial_feature_names) #get_dummies is the method\n",
    "\n",
    "# view newly engineering features\n",
    "\n",
    "print(training_features)\n",
    "\n",
    "# We have converted our categoricial data into numeric. \n",
    "# or we can say we have done feature engineering over categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OverallGrade_C', 'OverallGrade_F', 'OverallGrade_A', 'OverallGrade_B', 'Obedient_N', 'Obedient_Y', 'OverallGrade_E']\n"
     ]
    }
   ],
   "source": [
    "#--get list of new categorical features\n",
    "categorical_engineered_features = list(set(training_features.columns) - set(numeric_feature_names))\n",
    "\n",
    "print(categorical_engineered_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 4 : Modeling\n",
    "-----------------\n",
    "We will now build a simple classification (supervised) model based on our feature set by using the logistic regression algorithm. The following code depicts how to build the supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')  \n",
    "\n",
    "#--fit the model\n",
    "lr = LogisticRegression() #object banaya\n",
    "model = lr.fit(training_features, np.array(outcome_labels['Recommend'])) \n",
    "# np.array() converts from dataframe to numeric array\n",
    "\n",
    "#--view model parameters\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# to suppress warnings (as in the above case)\n",
    "# import warnings; warnings.simplefilter('ignore')  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thus, we now have our supervised learning model based on the logistic regression model with L2 regularization, as you can see from the parameters in the above output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 5 : Model Evaluation\n",
    "-------------------------\n",
    "Typically model evaluation is done based on some validation dataset that is different from the training dataset to prevent overfitting or biasing the model. Since this is an example on a toy dataset, let’s evaluate the performance of our model on the training data using the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 %\n",
      "Classification Stats:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       1.00      1.00      1.00         5\n",
      "         Yes       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--simple evaluation on training data\n",
    "pred_labels = model.predict(training_features)\n",
    "actual_labels = np.array(outcome_labels['Recommend'])\n",
    "\n",
    "#--evaluate model performance\n",
    "from sklearn.metrics import accuracy_score    \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Accuracy:', float(accuracy_score(actual_labels, pred_labels))*100, '%')\n",
    "# SYNTAX: accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
    "\n",
    "print('Classification Stats:')\n",
    "print(classification_report(actual_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUST WATCH VIDEO TO UNDERSTAND \"CLASSIFICATION REPORT\"\n",
    "--\n",
    ">https://www.youtube.com/watch?v=HBi-P5j0Kec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn Metrics\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 6 : Model Deployment     \n",
    "-------------------------\n",
    "We built our first supervised learning model, and to deploy this model typically in a system or server, we need to persist the model. We also need to save the scalar object we used to scale the numerical features since we use it to transform the numeric features of new data samples. The following snippet depicts a way to store the model and scalar objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaler/scaler.pickle']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--Model Deployment  -- optional in our case\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "#--save models to be deployed on your server\n",
    "if not os.path.exists('Model'):\n",
    "    os.mkdir('Model') #make directory (mkdr)\n",
    "if not os.path.exists('Scaler'):\n",
    "    os.mkdir('Scaler') \n",
    "    \n",
    "joblib.dump(model, r'Model/model.pickle') #Create a Pickle file->i.e  Model is convert into encrypted compressed binary form\n",
    "joblib.dump(ss, r'Scaler/scaler.pickle')\n",
    "\n",
    "# Check both the folders under  C:\\Program Files\\Python36"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 7 : Prediction in Action\n",
    "-----------------------------\n",
    "We are now ready to start predicting with our newly built and deployed model! To start predictions, we need to load our model and scalar objects into memory. The following code helps us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name OverallGrade Obedient  ResearchScore  ProjectScore\n",
      "0   Ninad            F        N             30            20\n",
      "1  Thomas            A        Y             78            80\n"
     ]
    }
   ],
   "source": [
    "#--Prediction in Action\n",
    "#--load model and scaler objects\n",
    "model = joblib.load(r'Model/model.pickle')\n",
    "scaler = joblib.load(r'Scaler/scaler.pickle')\n",
    "\n",
    "# We have some sample new student records (for two students) \n",
    "# for which we want our model to predict if they will get the \n",
    "# grant recommendation. \n",
    "# Let’s retrieve and view this data using the following code.\n",
    "\n",
    "#--data retrieval\n",
    "new_data = pd.DataFrame([{'Name': 'Ninad', 'OverallGrade': 'F', 'Obedient': 'N', 'ResearchScore': 30, 'ProjectScore': 20},\n",
    "                  {'Name': 'Thomas', 'OverallGrade': 'A', 'Obedient': 'Y', 'ResearchScore': 78, 'ProjectScore': 80}])\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_features\n",
      "   OverallGrade Obedient  ResearchScore  ProjectScore\n",
      "0            F        N             30            20\n",
      "1            A        Y             78            80\n",
      "   ResearchScore  ProjectScore  OverallGrade_A  OverallGrade_F  Obedient_N  \\\n",
      "0      -1.127647     -1.430636               0               1           1   \n",
      "1       0.494137      1.160705               1               0           0   \n",
      "\n",
      "   Obedient_Y  \n",
      "0           0  \n",
      "1           1  \n"
     ]
    }
   ],
   "source": [
    "# w.r.t new data\n",
    "# We will now carry out the tasks relevant to \n",
    "# data preparation—feature extraction, engineering, and scaling \n",
    "# in the following code snippet.\n",
    "\n",
    "#--data preparation\n",
    "prediction_features = new_data[feature_names]\n",
    "print(\"prediction_features\\n\",prediction_features)\n",
    "#--scaling\n",
    "prediction_features[numeric_feature_names] = scaler.transform(prediction_features[numeric_feature_names])\n",
    "\n",
    "#--engineering categorical variables\n",
    "prediction_features = pd.get_dummies(prediction_features, columns=categoricial_feature_names)\n",
    "\n",
    "#--view feature set\n",
    "print(prediction_features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now have the relevant features for the new students! However you can see that some of the categorical features are missing based on some grades like B, C, and E. This is because none of these students obtained those grades but we still need those attributes because the model was trained on all attributes including these. The following snippet helps us identify and add the missing categorical features.\n",
    "\n",
    "We add the value for each of those features as 0 for each student since they did not obtain those grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResearchScore  ProjectScore  OverallGrade_A  OverallGrade_F  Obedient_N  \\\n",
      "0      -1.127647     -1.430636               0               1           1   \n",
      "1       0.494137      1.160705               1               0           0   \n",
      "\n",
      "   Obedient_Y  OverallGrade_B  OverallGrade_E  OverallGrade_C  \n",
      "0           0               0               0               0  \n",
      "1           1               0               0               0  \n"
     ]
    }
   ],
   "source": [
    "# add missing categorical feature columns\n",
    "current_categorical_engineered_features = set(prediction_features.columns) - set(numeric_feature_names)\n",
    "\n",
    "missing_features = set(categorical_engineered_features) - current_categorical_engineered_features\n",
    "\n",
    "for feature in missing_features:\n",
    "    # add zeros since feature is absent in these data samples\n",
    "    prediction_features[feature] = [0] * len(prediction_features) #missing columns ke values ko 0 kiye\n",
    "    \n",
    "\n",
    "\n",
    "# view final feature set\n",
    "print(prediction_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name OverallGrade Obedient  ResearchScore  ProjectScore Recommend\n",
      "0   Ninad            F        N             30            20        No\n",
      "1  Thomas            A        Y             78            80       Yes\n"
     ]
    }
   ],
   "source": [
    "# We have our complete feature set ready for both the new students. \n",
    "# Let’s put our model to the test and get the predictions \n",
    "# with regard to grant recommendations!\n",
    "\n",
    "predictions = model.predict(prediction_features)\n",
    "\n",
    "##--display results\n",
    "new_data['Recommend'] = predictions\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can clearly see from above steps that our model has predicted grant recommendation labels for both the new students. \n",
    "\n",
    "Thomas clearly being diligent, having a straight A average and decent scores, is most likely to get the grant recommendation as compared to Ninad. Thus you can see that our model has learned how to predict grant recommendation outcomes based on past historical student data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Q n A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q1> What is overfitting or biasing ?\n",
    "\n",
    "Ans: Whenever working on a data set to predict or classify a problem, we tend to find accuracy by implementing a design model on first train set, then on test set. If the accuracy is satisfactory, we tend to increase accuracy of data-sets prediction either by increasing or decreasing data feature or applying feature engineering in our machine learning model. But sometime our model maybe giving poor result. \n",
    "\n",
    "The poor performance of our model maybe because, the model is too simple to describe the target, or may be model is too complex to express the target. \n",
    "\n",
    "See these two below figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Underfitting overfitting image](datasets_n_images/images/underfitting_overfitting_image_1.png 'underfitting overfitting image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Another Underfitting Overfitting Image](datasets_n_images/images/underfitting_overfitting_image_2.png 'Another Underfitting Overfitting image')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By looking at the graph on the left side we can predict that the line does not cover all the points shown in the graph. Such model tend to cause underfitting of data. It also called High Bias.\n",
    "\n",
    "Where as the graph on rightmost side, shows the predicted line covers all the points in graph. In such condition you can also think that it’s a good graph which cover all the points. But that’s not actually true, the predicted line into the graph covers all points which are noise and outlier. Such model are also responsible to predict poor result due to its complexity.It is also called High Variance.\n",
    "\n",
    "The middle graph has low bias and low variance. So in order to solve the problem of our model that is overfitting and underfitting we have to generalize our model, such that, it gives accurate predictions for all varieties of unseen data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q2> Under model evaluation what does precision , accuracy , F1 score , support indicate (in simplest terms) ?\n",
    "\n",
    "Ans: Read this simple detailed article for complete understanding\n",
    "https://medium.com/datadriveninvestor/accuracy-trap-pay-attention-to-recall-precision-f-score-auc-d02f28d3299c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q3> How standardscalar works ? // ss.transform()\n",
    "Ans : https://stackoverflow.com/questions/40758562/can-anyone-explain-me-standardscaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
